% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimization.R, R/optimization_original.R
\name{compute_objective}
\alias{compute_objective}
\title{Compute Objective Function for Spherical Single-Index Model with Group Lasso}
\usage{
compute_objective(
  beta,
  Theta,
  X,
  Y,
  knots,
  degree,
  Omega,
  lambda,
  gamma,
  group_idx,
  weights
)

compute_objective(
  beta,
  Theta,
  X,
  Y,
  knots,
  degree,
  Omega,
  lambda,
  gamma,
  group_idx,
  weights
)
}
\arguments{
\item{beta}{Index parameter vector (length p)}

\item{Theta}{B-spline coefficient matrix for link function approximation (n_basis × q_minus_1)}

\item{X}{Design matrix (n × p)}

\item{Y}{Response matrix on unit sphere (n × q)}

\item{knots}{Vector of B-spline knots for link function approximation}

\item{degree}{Degree of B-spline basis (typically 3 for cubic splines)}

\item{Omega}{Penalty matrix for roughness control (n_basis × n_basis)}

\item{lambda}{Regularization parameter for group lasso penalty}

\item{gamma}{Regularization parameter for link function roughness}

\item{group_idx}{List of group indices}

\item{weights}{Vector of group-specific weights (length G)}
}
\value{
Scalar value of the objective function

Scalar value of the objective function
}
\description{
Computes the complete objective function including prediction loss, link function
roughness penalty, and group lasso penalty for sparse index parameter estimation.

Computes the complete objective function including prediction loss, link function
roughness penalty, and group lasso penalty for sparse index parameter estimation.
}
\details{
The objective function is:
\deqn{Q(β, Θ) = L(β, Θ) + γR(Θ) + λP(β)}

where:
\describe{
  \item{\strong{Loss term:}}{L(β, Θ) = (1/n) Σ_i ||Y_i - m(X_i^T β)||^2,
        measuring prediction error on the sphere}
  \item{\strong{Roughness penalty:}}{R(Θ) = tr(Θ^T Ω Θ),
        controlling smoothness of the link function via the penalty matrix Ω}
  \item{\strong{Group lasso penalty:}}{P(β) = Σ_g w_g ||β_g||_2,
        inducing group-level sparsity in the index parameter}
}

\strong{Computational Steps:}
\enumerate{
  \item Compute single index: z_i = X_i^T β
  \item Clip z values to valid knot range to avoid extrapolation
  \item Evaluate B-spline basis: B(z_i)
  \item Compute link function in R^{q-1}: U_i = B(z_i) Θ
  \item Map to sphere via inverse stereographic projection: Ŷ_i = π^{-1}(U_i)
  \item Calculate squared error: ||Y_i - Ŷ_i||^2
  \item Add roughness penalty: γ tr(Θ^T Ω Θ)
  \item Add group lasso penalty: λ Σ_g w_g ||β_g||_2
}

\strong{Parameter Roles:}
\itemize{
  \item λ controls sparsity: larger values select fewer groups
  \item γ controls smoothness: larger values produce smoother link functions
  \item weights allow adaptive penalties: typically w_g ∝ ||β_init_g||^{-γ}
}

The objective function is:
\deqn{Q(β, Θ) = L(β, Θ) + γR(Θ) + λP(β)}

where:
\describe{
  \item{\strong{Loss term:}}{L(β, Θ) = (1/n) Σ_i ||Y_i - m(X_i^T β)||^2,
        measuring prediction error on the sphere}
  \item{\strong{Roughness penalty:}}{R(Θ) = tr(Θ^T Ω Θ),
        controlling smoothness of the link function via the penalty matrix Ω}
  \item{\strong{Group lasso penalty:}}{P(β) = Σ_g w_g ||β_g||_2,
        inducing group-level sparsity in the index parameter}
}

\strong{Computational Steps:}
\enumerate{
  \item Compute single index: z_i = X_i^T β
  \item Clip z values to valid knot range to avoid extrapolation
  \item Evaluate B-spline basis: B(z_i)
  \item Compute link function in R^{q-1}: U_i = B(z_i) Θ
  \item Map to sphere via inverse stereographic projection: Ŷ_i = π^{-1}(U_i)
  \item Calculate squared error: ||Y_i - Ŷ_i||^2
  \item Add roughness penalty: γ tr(Θ^T Ω Θ)
  \item Add group lasso penalty: λ Σ_g w_g ||β_g||_2
}

\strong{Parameter Roles:}
\itemize{
  \item λ controls sparsity: larger values select fewer groups
  \item γ controls smoothness: larger values produce smoother link functions
  \item weights allow adaptive penalties: typically w_g ∝ ||β_init_g||^{-γ}
}
}
\examples{
\dontrun{
# Generate synthetic data
set.seed(123)
data <- generate_spherical_data(n = 100, p = 20, G = 4)

# Setup for objective computation
group_idx <- data$group_idx
weights <- sapply(group_idx, function(idx) sqrt(length(idx)))

# B-spline setup
n_knots <- 5
z_range <- range(data$X \%*\% data$beta_true)
knots <- seq(z_range[1] - 0.1, z_range[2] + 0.1, length.out = n_knots + 2)
full_knots <- c(rep(knots[1], 4), knots, rep(knots[length(knots)], 4))

# Initialize parameters
beta <- data$beta_true + rnorm(20, 0, 0.1)
beta <- beta / sqrt(sum(beta^2))

n_basis <- n_knots + 4
Theta <- matrix(rnorm(n_basis * 2), n_basis, 2)

# Penalty matrix for roughness
D2 <- diff(diag(n_basis), differences = 2)
Omega <- t(D2) \%*\% D2

# Compute objective for different lambda values
lambdas <- c(0.01, 0.1, 0.5, 1.0)
objectives <- sapply(lambdas, function(lam) {
  compute_objective(beta, Theta, data$X, data$Y,
                   full_knots, degree = 3, Omega,
                   lambda = lam, gamma = 0.1,
                   group_idx, weights)
})

plot(lambdas, objectives, type = "b", log = "x",
     xlab = "Lambda", ylab = "Objective Value",
     main = "Objective vs Regularization")

# Decompose objective into components
lambda <- 0.1
gamma <- 0.1

# Full objective
obj_full <- compute_objective(beta, Theta, data$X, data$Y,
                              full_knots, 3, Omega,
                              lambda, gamma, group_idx, weights)

# Loss only (lambda = gamma = 0)
obj_loss <- compute_objective(beta, Theta, data$X, data$Y,
                              full_knots, 3, Omega,
                              lambda = 0, gamma = 0, group_idx, weights)

# Loss + roughness (lambda = 0)
obj_smooth <- compute_objective(beta, Theta, data$X, data$Y,
                                full_knots, 3, Omega,
                                lambda = 0, gamma, group_idx, weights)

cat("Loss component:", obj_loss, "\n")
cat("+ Roughness:", obj_smooth - obj_loss, "\n")
cat("+ Group penalty:", obj_full - obj_smooth, "\n")
cat("Total objective:", obj_full, "\n")
}

\dontrun{
# Generate synthetic data
set.seed(123)
data <- generate_spherical_data(n = 100, p = 20, G = 4)

# Setup for objective computation
group_idx <- data$group_idx
weights <- sapply(group_idx, function(idx) sqrt(length(idx)))

# B-spline setup
n_knots <- 5
z_range <- range(data$X \%*\% data$beta_true)
knots <- seq(z_range[1] - 0.1, z_range[2] + 0.1, length.out = n_knots + 2)
full_knots <- c(rep(knots[1], 4), knots, rep(knots[length(knots)], 4))

# Initialize parameters
beta <- data$beta_true + rnorm(20, 0, 0.1)
beta <- beta / sqrt(sum(beta^2))

n_basis <- n_knots + 4
Theta <- matrix(rnorm(n_basis * 2), n_basis, 2)

# Penalty matrix for roughness
D2 <- diff(diag(n_basis), differences = 2)
Omega <- t(D2) \%*\% D2

# Compute objective for different lambda values
lambdas <- c(0.01, 0.1, 0.5, 1.0)
objectives <- sapply(lambdas, function(lam) {
  compute_objective(beta, Theta, data$X, data$Y,
                   full_knots, degree = 3, Omega,
                   lambda = lam, gamma = 0.1,
                   group_idx, weights)
})

plot(lambdas, objectives, type = "b", log = "x",
     xlab = "Lambda", ylab = "Objective Value",
     main = "Objective vs Regularization")

# Decompose objective into components
lambda <- 0.1
gamma <- 0.1

# Full objective
obj_full <- compute_objective(beta, Theta, data$X, data$Y,
                              full_knots, 3, Omega,
                              lambda, gamma, group_idx, weights)

# Loss only (lambda = gamma = 0)
obj_loss <- compute_objective(beta, Theta, data$X, data$Y,
                              full_knots, 3, Omega,
                              lambda = 0, gamma = 0, group_idx, weights)

# Loss + roughness (lambda = 0)
obj_smooth <- compute_objective(beta, Theta, data$X, data$Y,
                                full_knots, 3, Omega,
                                lambda = 0, gamma, group_idx, weights)

cat("Loss component:", obj_loss, "\n")
cat("+ Roughness:", obj_smooth - obj_loss, "\n")
cat("+ Group penalty:", obj_full - obj_smooth, "\n")
cat("Total objective:", obj_full, "\n")
}

}
\seealso{
\code{\link{grad_beta}} for gradient computation,
\code{\link{spherical_sim_group}} for optimization using this objective,
\code{\link{inv_stereo}} for inverse stereographic projection

\code{\link{grad_beta}} for gradient computation,
\code{\link{spherical_sim_group}} for optimization using this objective,
\code{\link{inv_stereo}} for inverse stereographic projection
}
