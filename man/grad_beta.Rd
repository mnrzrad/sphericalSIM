% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimization.R, R/optimization_original.R
\name{grad_beta}
\alias{grad_beta}
\title{Compute Gradient of Loss with Respect to Index Parameter}
\usage{
grad_beta(beta, Theta, X, Y, knots, degree)

grad_beta(beta, Theta, X, Y, knots, degree)
}
\arguments{
\item{beta}{Index parameter vector (length p)}

\item{Theta}{B-spline coefficient matrix for link function (n_basis × q_minus_1)}

\item{X}{Design matrix (n × p)}

\item{Y}{Response matrix on unit sphere (n × q)}

\item{knots}{Vector of B-spline knots}

\item{degree}{Degree of B-spline basis (typically 3 for cubic splines)}
}
\value{
Gradient vector of length p: ∇_β L(β)

Gradient vector of length p: ∇_β L(β)
}
\description{
Computes the gradient of the spherical prediction loss with respect to the
index parameter β using the chain rule through the B-spline basis and
inverse stereographic projection.

Computes the gradient of the spherical prediction loss with respect to the
index parameter β using the chain rule through the B-spline basis and
inverse stereographic projection.
}
\details{
For the loss function L(β) = (1/n) Σ_i ||Y_i - m(X_i^T β)||^2, the gradient is:
\deqn{∇_β L = (2/n) X^T t}
where t_i = (Y_i - Ŷ_i)^T (∂Ŷ_i/∂z_i) with z_i = X_i^T β.

\strong{Chain Rule Decomposition:}
The derivative ∂Ŷ_i/∂z_i is computed via the chain rule:
\deqn{∂Ŷ_i/∂z_i = (∂π^{-1}/∂u)(U_i) · (∂u/∂z)(z_i)}
where:
\itemize{
  \item π^{-1} is the inverse stereographic projection
  \item (∂π^{-1}/∂u)(U_i) is the Jacobian matrix (q × (q-1)) computed by
        \code{\link{jacobian_inv_stereo}}
  \item (∂u/∂z)(z_i) = B'(z_i) Θ where B'(z_i) is the derivative of the
        B-spline basis
}

\strong{Computational Steps:}
\enumerate{
  \item Compute single index: z_i = X_i^T β
  \item Clip z to valid knot range
  \item Evaluate B-spline basis B(z_i) and its derivative B'(z_i)
  \item Compute link function: U_i = B(z_i) Θ
  \item Map to sphere: Ŷ_i = π^{-1}(U_i)
  \item Compute residuals: E_i = Ŷ_i - Y_i
  \item For each i, compute Jacobian J_i = (∂π^{-1}/∂u)(U_i)
  \item Compute link derivative: m'_i = B'(z_i) Θ
  \item Apply chain rule: t_i = E_i^T · (J_i · m'_i)
  \item Aggregate: ∇L = (2/n) X^T t
}

\strong{Usage in Optimization:}
This gradient is used in the proximal gradient algorithm:
\enumerate{
  \item Gradient step: β_temp = β - step * grad_beta(β, Θ, X, Y, ...)
  \item Proximal step: β_new = prox_group_lasso(β_temp, ...)
}

The gradient computation does not include the group lasso penalty because
it is handled separately via the proximal operator.

For the loss function L(β) = (1/n) Σ_i ||Y_i - m(X_i^T β)||^2, the gradient is:
\deqn{∇_β L = (2/n) X^T t}
where t_i = (Y_i - Ŷ_i)^T (∂Ŷ_i/∂z_i) with z_i = X_i^T β.

\strong{Chain Rule Decomposition:}
The derivative ∂Ŷ_i/∂z_i is computed via the chain rule:
\deqn{∂Ŷ_i/∂z_i = (∂π^{-1}/∂u)(U_i) · (∂u/∂z)(z_i)}
where:
\itemize{
  \item π^{-1} is the inverse stereographic projection
  \item (∂π^{-1}/∂u)(U_i) is the Jacobian matrix (q × (q-1)) computed by
        \code{\link{jacobian_inv_stereo}}
  \item (∂u/∂z)(z_i) = B'(z_i) Θ where B'(z_i) is the derivative of the
        B-spline basis
}

\strong{Computational Steps:}
\enumerate{
  \item Compute single index: z_i = X_i^T β
  \item Clip z to valid knot range
  \item Evaluate B-spline basis B(z_i) and its derivative B'(z_i)
  \item Compute link function: U_i = B(z_i) Θ
  \item Map to sphere: Ŷ_i = π^{-1}(U_i)
  \item Compute residuals: E_i = Ŷ_i - Y_i
  \item For each i, compute Jacobian J_i = (∂π^{-1}/∂u)(U_i)
  \item Compute link derivative: m'_i = B'(z_i) Θ
  \item Apply chain rule: t_i = E_i^T · (J_i · m'_i)
  \item Aggregate: ∇L = (2/n) X^T t
}

\strong{Usage in Optimization:}
This gradient is used in the proximal gradient algorithm:
\enumerate{
  \item Gradient step: β_temp = β - step * grad_beta(β, Θ, X, Y, ...)
  \item Proximal step: β_new = prox_group_lasso(β_temp, ...)
}

The gradient computation does not include the group lasso penalty because
it is handled separately via the proximal operator.
}
\examples{
\dontrun{
# Generate synthetic data
set.seed(123)
data <- generate_spherical_data(n = 100, p = 20)

# B-spline setup
n_knots <- 5
z_range <- range(data$X \%*\% data$beta_true)
knots <- seq(z_range[1] - 0.1, z_range[2] + 0.1, length.out = n_knots + 2)
full_knots <- c(rep(knots[1], 4), knots, rep(knots[length(knots)], 4))

# Initialize parameters
beta <- rnorm(20)
beta <- beta / sqrt(sum(beta^2))

n_basis <- n_knots + 4
Theta <- matrix(rnorm(n_basis * 2), n_basis, 2)

# Compute gradient
grad <- grad_beta(beta, Theta, data$X, data$Y, full_knots, degree = 3)

cat("Gradient norm:", sqrt(sum(grad^2)), "\n")
cat("Gradient range: [", min(grad), ",", max(grad), "]\n")

# Verify gradient numerically
eps <- 1e-6
grad_numerical <- numeric(length(beta))

for (j in 1:length(beta)) {
  beta_plus <- beta
  beta_plus[j] <- beta[j] + eps

  beta_minus <- beta
  beta_minus[j] <- beta[j] - eps

  # Compute loss at both points
  loss_plus <- mean(rowSums((data$Y - inv_stereo(
    splineDesign(full_knots, as.vector(data$X \%*\% beta_plus), 4) \%*\% Theta
  ))^2))

  loss_minus <- mean(rowSums((data$Y - inv_stereo(
    splineDesign(full_knots, as.vector(data$X \%*\% beta_minus), 4) \%*\% Theta
  ))^2))

  grad_numerical[j] <- (loss_plus - loss_minus) / (2 * eps)
}

# Compare analytical vs numerical gradient
plot(grad, grad_numerical, asp = 1,
     xlab = "Analytical Gradient", ylab = "Numerical Gradient",
     main = "Gradient Verification")
abline(0, 1, col = "red", lty = 2)

cat("Max absolute difference:", max(abs(grad - grad_numerical)), "\n")
cat("Relative error:",
    max(abs(grad - grad_numerical)) / max(abs(grad)), "\n")

# Visualize gradient along coordinates
barplot(grad, main = "Gradient Components",
        xlab = "Coefficient Index", ylab = "Gradient Value")
abline(h = 0, lty = 2)
}

\dontrun{
# Generate synthetic data
set.seed(123)
data <- generate_spherical_data(n = 100, p = 20)

# B-spline setup
n_knots <- 5
z_range <- range(data$X \%*\% data$beta_true)
knots <- seq(z_range[1] - 0.1, z_range[2] + 0.1, length.out = n_knots + 2)
full_knots <- c(rep(knots[1], 4), knots, rep(knots[length(knots)], 4))

# Initialize parameters
beta <- rnorm(20)
beta <- beta / sqrt(sum(beta^2))

n_basis <- n_knots + 4
Theta <- matrix(rnorm(n_basis * 2), n_basis, 2)

# Compute gradient
grad <- grad_beta(beta, Theta, data$X, data$Y, full_knots, degree = 3)

cat("Gradient norm:", sqrt(sum(grad^2)), "\n")
cat("Gradient range: [", min(grad), ",", max(grad), "]\n")

# Verify gradient numerically
eps <- 1e-6
grad_numerical <- numeric(length(beta))

for (j in 1:length(beta)) {
  beta_plus <- beta
  beta_plus[j] <- beta[j] + eps

  beta_minus <- beta
  beta_minus[j] <- beta[j] - eps

  # Compute loss at both points
  loss_plus <- mean(rowSums((data$Y - inv_stereo(
    splineDesign(full_knots, as.vector(data$X \%*\% beta_plus), 4) \%*\% Theta
  ))^2))

  loss_minus <- mean(rowSums((data$Y - inv_stereo(
    splineDesign(full_knots, as.vector(data$X \%*\% beta_minus), 4) \%*\% Theta
  ))^2))

  grad_numerical[j] <- (loss_plus - loss_minus) / (2 * eps)
}

# Compare analytical vs numerical gradient
plot(grad, grad_numerical, asp = 1,
     xlab = "Analytical Gradient", ylab = "Numerical Gradient",
     main = "Gradient Verification")
abline(0, 1, col = "red", lty = 2)

cat("Max absolute difference:", max(abs(grad - grad_numerical)), "\n")
cat("Relative error:",
    max(abs(grad - grad_numerical)) / max(abs(grad)), "\n")

# Visualize gradient along coordinates
barplot(grad, main = "Gradient Components",
        xlab = "Coefficient Index", ylab = "Gradient Value")
abline(h = 0, lty = 2)
}

}
\references{
- de Boor, C. (2001). A Practical Guide to Splines. Springer.
- Parikh, N., & Boyd, S. (2014). Proximal algorithms. Foundations and Trends
  in Optimization, 1(3), 127-239.

- de Boor, C. (2001). A Practical Guide to Splines. Springer.
- Parikh, N., & Boyd, S. (2014). Proximal algorithms. Foundations and Trends
  in Optimization, 1(3), 127-239.
}
\seealso{
\code{\link{compute_objective}} for the full objective function,
\code{\link{prox_group_lasso}} for the proximal operator,
\code{\link{jacobian_inv_stereo}} for Jacobian computation,
\code{\link{spherical_sim_group}} for the complete optimization algorithm

\code{\link{compute_objective}} for the full objective function,
\code{\link{prox_group_lasso}} for the proximal operator,
\code{\link{jacobian_inv_stereo}} for Jacobian computation,
\code{\link{spherical_sim_group}} for the complete optimization algorithm
}
