% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/cross_validation.R
\name{cv_lambda_path_early_stop}
\alias{cv_lambda_path_early_stop}
\title{Cross-Validation for Lambda with Early Stopping}
\usage{
cv_lambda_path_early_stop(
  X,
  Y,
  group_idx,
  lambda_seq,
  gamma,
  weights = NULL,
  n_folds = 5,
  n_knots = 10,
  patience = 3,
  min_groups = 1,
  max_groups = NULL,
  min_valid_folds = 2,
  verbose = FALSE,
  ...
)
}
\arguments{
\item{X}{Design matrix (n  x  p)}

\item{Y}{Response matrix on unit sphere (n  x  q)}

\item{group_idx}{List of length G, where each element contains indices of predictors in that group}

\item{lambda_seq}{Vector of lambda values to evaluate (typically in decreasing order)}

\item{gamma}{Roughness penalty parameter for link function smoothness}

\item{weights}{Vector of group-specific weights (length G). If NULL, uses sqrt(group_size) (default NULL)}

\item{n_folds}{Number of cross-validation folds (default 5)}

\item{n_knots}{Number of internal B-spline knots for link function approximation (default 10)}

\item{patience}{Number of lambda values without improvement before early stopping (default 3)}

\item{min_groups}{Minimum number of groups that must be selected (default 1)}

\item{max_groups}{Maximum number of groups allowed to be selected. If NULL, uses G (default NULL)}

\item{min_valid_folds}{Minimum number of folds with valid fits required for a lambda to be considered (default 2)}

\item{verbose}{Print detailed progress information (default FALSE)}

\item{...}{Additional arguments passed to \code{\link{spherical_sim_group}}}
}
\value{
List with the following components:
\describe{
  \item{lambda}{Vector of evaluated lambda values (may be shorter than lambda_seq due to early stopping)}
  \item{cv_error}{Vector of CV errors for each evaluated lambda (Inf for invalid)}
  \item{n_selected}{Vector of average number of groups selected across folds}
  \item{n_valid_folds}{Vector indicating number of valid folds for each lambda}
  \item{best_lambda}{Lambda value with minimum CV error}
  \item{best_error}{Minimum CV error achieved}
  \item{best_idx}{Index of best_lambda in the lambda vector}
  \item{n_evaluated}{Total number of lambda values evaluated}
  \item{n_valid}{Number of lambda values with finite CV error}
}
}
\description{
Performs k-fold cross-validation over a sequence of lambda values with early
stopping to efficiently select the optimal regularization parameter for group
lasso in spherical single-index models. Includes constraints on group selection
and robust error handling.
}
\details{
This function efficiently searches for the optimal lambda by evaluating a sequence
of lambda values and stopping early when no improvement is observed.

\strong{Cross-Validation Procedure:}
For each lambda value:
\enumerate{
  \item Split data into n_folds folds
  \item For each fold k:
    \itemize{
      \item Fit model on training folds (all except k)
      \item Predict on test fold k
      \item Compute prediction error if number of selected groups is in [min_groups, max_groups]
    }
  \item Average prediction errors across valid folds
  \item Track best lambda and check for early stopping
}

\strong{Early Stopping Criteria:}
The search stops when any of these conditions is met:
\itemize{
  \item No improvement in CV error for 'patience' consecutive lambda values
  \item Too many consecutive failed fold evaluations
  \item Average number of selected groups exceeds max_groups
  \item All lambda values have been evaluated
}

\strong{Robust Error Handling:}
\itemize{
  \item If a fold fails to fit, that fold contributes Inf to CV error
  \item If a fold selects too many/few groups, it contributes Inf
  \item Lambda values with fewer than min_valid_folds successful folds get CV error = Inf
  \item Gracefully handles numerical issues in fitting and prediction
}

\strong{Lambda Sequence Strategy:}
For best results, provide lambda_seq in decreasing order (large to small):
\itemize{
  \item Large lambda: sparse models (few groups), fast fitting
  \item Small lambda: dense models (many groups), slower fitting
  \item Early stopping saves time when CV error stops improving
}

\strong{Group Selection Constraints:}
The min_groups and max_groups parameters allow you to:
\itemize{
  \item Avoid degenerate solutions (min_groups = 1)
  \item Control model complexity (max_groups < G)
  \item Focus search on interpretable models (max_groups = 3 or 5)
}
}
\examples{
\dontrun{
# Generate data
set.seed(123)
data <- generate_spherical_data(n = 200, p = 20, G = 4, seed = 123)

# Create lambda sequence (decreasing)
lambda_max <- 1.0
lambda_seq <- exp(seq(log(lambda_max), log(lambda_max * 0.01), length.out = 20))

# Basic cross-validation
cv_result <- cv_lambda_path_early_stop(
  X = data$X,
  Y = data$Y,
  group_idx = data$group_idx,
  lambda_seq = lambda_seq,
  gamma = 0.1,
  n_folds = 5,
  verbose = TRUE
)

# Examine results
cat("Best lambda:", cv_result$best_lambda, "\n")
cat("Best CV error:", cv_result$best_error, "\n")
cat("Evaluated:", cv_result$n_evaluated, "out of", length(lambda_seq), "lambdas\n")

# Plot CV curve
finite_idx <- is.finite(cv_result$cv_error)
plot(cv_result$lambda[finite_idx], cv_result$cv_error[finite_idx],
     type = "b", log = "x", pch = 19,
     xlab = "Lambda", ylab = "CV Error",
     main = "Cross-Validation Curve")
abline(v = cv_result$best_lambda, col = "red", lty = 2)

# Plot number of selected groups
plot(cv_result$lambda[finite_idx], cv_result$n_selected[finite_idx],
     type = "b", log = "x", pch = 19,
     xlab = "Lambda", ylab = "Number of Selected Groups",
     main = "Model Sparsity vs Lambda")

# CV with group constraints
cv_constrained <- cv_lambda_path_early_stop(
  X = data$X,
  Y = data$Y,
  group_idx = data$group_idx,
  lambda_seq = lambda_seq,
  gamma = 0.1,
  min_groups = 2,
  max_groups = 3,  # Force selection of 2-3 groups
  n_folds = 5,
  verbose = TRUE
)

# CV with adaptive weights
# First compute adaptive weights
initial_fit <- spherical_sim_group(
  X = data$X, Y = data$Y,
  group_idx = data$group_idx,
  lambda = 0.05, gamma = 0.1,
  verbose = FALSE
)
adaptive_weights <- sapply(data$group_idx, function(idx) {
  norm <- sqrt(sum(initial_fit$beta[idx]^2))
  sqrt(length(idx)) / max(norm, 1e-3)
})

cv_adaptive <- cv_lambda_path_early_stop(
  X = data$X,
  Y = data$Y,
  group_idx = data$group_idx,
  lambda_seq = lambda_seq,
  gamma = 0.1,
  weights = adaptive_weights,
  n_folds = 5,
  verbose = TRUE
)

# Compare standard vs adaptive
par(mfrow = c(1, 2))
plot(cv_result$lambda, cv_result$cv_error, type = "b", log = "x",
     main = "Standard Weights", xlab = "Lambda", ylab = "CV Error")
plot(cv_adaptive$lambda, cv_adaptive$cv_error, type = "b", log = "x",
     main = "Adaptive Weights", xlab = "Lambda", ylab = "CV Error")

# Check early stopping effectiveness
cat("Standard evaluated:", cv_result$n_evaluated, "/", length(lambda_seq), "\n")
cat("Adaptive evaluated:", cv_adaptive$n_evaluated, "/", length(lambda_seq), "\n")

# Examine fold stability
fold_success_rate <- cv_result$n_valid_folds / 5
plot(cv_result$lambda, fold_success_rate, type = "b", log = "x",
     xlab = "Lambda", ylab = "Fraction of Valid Folds",
     main = "Fold Success Rate",
     ylim = c(0, 1))
abline(h = 0.4, col = "red", lty = 2)  # min_valid_folds/n_folds threshold

# Fit final model with best lambda
final_fit <- spherical_sim_group(
  X = data$X,
  Y = data$Y,
  group_idx = data$group_idx,
  lambda = cv_result$best_lambda,
  gamma = 0.1,
  n_knots = 10,
  verbose = TRUE
)

cat("Selected groups:", final_fit$selected_groups, "\n")
cat("True active groups:", data$active_groups, "\n")
}

}
\references{
- Hastie, T., Tibshirani, R., & Friedman, J. (2009). The Elements of
  Statistical Learning (2nd ed.). Springer.
- Buhlmann, P., & van de Geer, S. (2011). Statistics for High-Dimensional
  Data: Methods, Theory and Applications. Springer.
}
\seealso{
\code{\link{cv_two_stage_adaptive}} for two-stage CV with gamma search,
\code{\link{spherical_sim_group}} for model fitting,
\code{\link{predict_spherical}} for predictions,
\code{compute_lambda_max_with_weights} for computing lambda_max
}
