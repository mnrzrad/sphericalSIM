% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/simulation.R
\name{summarize_results}
\alias{summarize_results}
\title{Summarize Simulation Study Results}
\usage{
summarize_results(results)
}
\arguments{
\item{results}{List of simulation results from \code{run_simulation_study()}
or multiple calls to \code{run_single_simulation()}}
}
\value{
List with two components:
\describe{
  \item{metrics}{Data frame with one row per simulation containing all extracted metrics}
  \item{summary}{Data frame with summary statistics (mean, SD, median) for each metric}
}

List with two components:
\describe{
  \item{metrics}{Data frame with one row per simulation containing all metrics}
  \item{summary}{Data frame with summary statistics for each metric category}
}
}
\description{
Computes summary statistics across multiple simulation replications and
prints formatted tables of performance metrics.
}
\details{
The function extracts key metrics from each simulation and computes:
\itemize{
  \item Mean, standard deviation, and median for continuous metrics
  \item Convergence rate (percentage of simulations that converged)
  \item Average number of iterations until convergence
}

Metrics summarized include:
\describe{
  \item{n_selected}{Number of groups selected by the model}
  \item{TPR}{True positive rate for group selection}
  \item{FPR}{False positive rate for group selection}
  \item{beta_l2_error}{L2 error for index parameter estimation}
  \item{angular_error}{Angular error between true and estimated beta (degrees)}
  \item{link_mse}{Mean squared error of link function estimation}
  \item{pred_error}{Prediction error on training data}
}

This function extracts or computes all evaluation metrics from Section 3:

\strong{Coefficient Metrics (Section 3.1):}
\itemize{
  \item MSE_beta, RMSE_beta, MAE_beta: Computed from beta_true and beta_est
  \item rho_beta: Cosine similarity (after sign normalization)
  \item angular_error: Angular error in degrees
}

\strong{Link Function Metrics (Section 3.2):}
\itemize{
  \item MSE_m, MAE_m: Computed from m_true and m_est
}

\strong{Predictive Performance (Section 3.3):}
\itemize{
  \item W1: 1D Wasserstein distance (computed from m_true and m_est)
  \item pred_error: Prediction error
}

\strong{Variable Selection Metrics (Section 3.4):}
\itemize{
  \item Precision, Recall, F1, FPR: Computed from selected_groups and true_active_groups
  \item TP, FP, FN, TN: Confusion matrix counts
}
}
\examples{
\dontrun{
# After running simulation study
results <- run_simulation_study(n_sim = 50)
summary <- summarize_results(results)

# Access detailed metrics
head(summary$metrics)

# Access summary statistics
print(summary$summary)

# Create custom plots
library(ggplot2)
ggplot(summary$metrics, aes(x = TPR)) +
  geom_histogram(bins = 20) +
  labs(title = "Distribution of True Positive Rates")

# Check relationship between metrics
with(summary$metrics, cor(TPR, angular_error))

# Load and summarize saved results
results <- readRDS("simulation_results.rds")
summary <- summarize_results(results)
}

}
\seealso{
\code{\link{run_simulation_study}} for running simulations,
\code{\link{run_single_simulation}} for individual simulation details
}
