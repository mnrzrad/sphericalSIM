% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/optimization.R, R/optimization_original.R
\name{prox_group_lasso}
\alias{prox_group_lasso}
\title{Proximal Operator for Group Lasso Penalty}
\usage{
prox_group_lasso(beta, group_idx, weights, lambda, step)

prox_group_lasso(beta, group_idx, weights, lambda, step)
}
\arguments{
\item{beta}{Current coefficient vector (length p)}

\item{group_idx}{List of length G, where each element contains indices of predictors in that group}

\item{weights}{Vector of group-specific weights (length G)}

\item{lambda}{Regularization parameter controlling overall sparsity level}

\item{step}{Step size (learning rate) for the proximal gradient algorithm}
}
\value{
Vector of updated coefficients (same length as beta) with group-level
  sparsity. Entire groups may be set to zero if their norm is below the threshold.

Vector of updated coefficients (same length as beta) with group-level
  sparsity. Entire groups may be set to zero if their norm is below the threshold.
}
\description{
Computes the proximal operator for the weighted group lasso penalty, which
performs soft-thresholding at the group level. This is a key component in
proximal gradient methods for group-sparse optimization.

Computes the proximal operator for the weighted group lasso penalty, which
performs soft-thresholding at the group level. This is a key component in
proximal gradient methods for group-sparse optimization.
}
\details{
The proximal operator for the weighted group lasso penalty is defined as:
\deqn{\text{prox}(β) = \arg\min_u \left\{ \frac{1}{2}||u - β||^2 + \text{step} · λ \sum_{g=1}^G w_g ||u_g||_2 \right\}}

The solution has a closed form for each group g:
\deqn{u_g = \left(1 - \frac{\text{step} · λ · w_g}{||β_g||_2}\right)_+ β_g}
where (x)_+ = max(0, x) is the positive part function.

\strong{Group-Level Soft-Thresholding:}
For each group g:
\itemize{
  \item Compute the L2 norm: ||β_g||_2 = sqrt(sum(β_g^2))
  \item Compute threshold: τ = step * λ * w_g
  \item If ||β_g||_2 > τ: shrink the group by factor (1 - τ/||β_g||_2)
  \item If ||β_g||_2 ≤ τ: set entire group to zero (group selection)
}

\strong{Role in Proximal Gradient Method:}
In the proximal gradient algorithm for solving:
\deqn{\min f(β) + λ \sum_g w_g ||β_g||_2}
each iteration performs:
\enumerate{
  \item Gradient step: β_temp = β - step * ∇f(β)
  \item Proximal step: β_new = prox_group_lasso(β_temp, ...)
}

\strong{Adaptive Weights:}
The weights w_g allow for adaptive group lasso, where w_g = ||β_init_g||^(-γ)
gives heavier penalties to groups with smaller initial estimates, encouraging
sparser solutions while protecting strong signals.

The proximal operator for the weighted group lasso penalty is defined as:
\deqn{\text{prox}(β) = \arg\min_u \left\{ \frac{1}{2}||u - β||^2 + \text{step} · λ \sum_{g=1}^G w_g ||u_g||_2 \right\}}

The solution has a closed form for each group g:
\deqn{u_g = \left(1 - \frac{\text{step} · λ · w_g}{||β_g||_2}\right)_+ β_g}
where (x)_+ = max(0, x) is the positive part function.

\strong{Group-Level Soft-Thresholding:}
For each group g:
\itemize{
  \item Compute the L2 norm: ||β_g||_2 = sqrt(sum(β_g^2))
  \item Compute threshold: τ = step * λ * w_g
  \item If ||β_g||_2 > τ: shrink the group by factor (1 - τ/||β_g||_2)
  \item If ||β_g||_2 ≤ τ: set entire group to zero (group selection)
}

\strong{Role in Proximal Gradient Method:}
In the proximal gradient algorithm for solving:
\deqn{\min f(β) + λ \sum_g w_g ||β_g||_2}
each iteration performs:
\enumerate{
  \item Gradient step: β_temp = β - step * ∇f(β)
  \item Proximal step: β_new = prox_group_lasso(β_temp, ...)
}

\strong{Adaptive Weights:}
The weights w_g allow for adaptive group lasso, where w_g = ||β_init_g||^(-γ)
gives heavier penalties to groups with smaller initial estimates, encouraging
sparser solutions while protecting strong signals.
}
\references{
- Yuan, M., & Lin, Y. (2006). Model selection and estimation in regression
  with grouped variables. Journal of the Royal Statistical Society: Series B,
  68(1), 49-67.
- Parikh, N., & Boyd, S. (2014). Proximal algorithms. Foundations and Trends
  in Optimization, 1(3), 127-239.
- Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of
  the American Statistical Association, 101(476), 1418-1429.

- Yuan, M., & Lin, Y. (2006). Model selection and estimation in regression
  with grouped variables. Journal of the Royal Statistical Society: Series B,
  68(1), 49-67.
- Parikh, N., & Boyd, S. (2014). Proximal algorithms. Foundations and Trends
  in Optimization, 1(3), 127-239.
- Zou, H. (2006). The adaptive lasso and its oracle properties. Journal of
  the American Statistical Association, 101(476), 1418-1429.
}
\seealso{
\code{\link{spherical_sim_group}} for the main optimization routine using this operator,
\code{\link{compute_adaptive_weights_fast}} for computing adaptive weights,
\code{\link{compute_adaptive_weights_grouplasso}} for alternative weight computation

\code{\link{spherical_sim_group}} for the main optimization routine using this operator,
\code{\link{compute_adaptive_weights_fast}} for computing adaptive weights,
\code{\link{compute_adaptive_weights_grouplasso}} for alternative weight computation
}
